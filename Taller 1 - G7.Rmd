---
title: "Taller 1 - BD&ML Grupo 7"
author: "Grupo 7"
date: "2025-02-02"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
gc(rm(list = ls()))
knitr::opts_chunk$set(echo = TRUE)
require(pacman)
p_load(stringr, # html to text
       tidyverse, # tidy-data
       skimr, # summary data
       rvest, # Web scrapping
       dplyr, # managing tables
       corrplot,#Correlation plot
       reshape2, # Long format for table 
       stargazer, # Latex tables
       gridExtra, # visualizing missing data
       furrr, 
       mosaic, # Bootstrap
       gridExtra, #multiple graphs
       boot,
       car,
       lmtest,
       caret, 
       sandwich,
       performance)
```


## 2. Data proccessing

### 2.a) Data description [On latex]

### 2.b) Web Scrapping [Explanation on latex]

<div>

```{r 2.b) Scrapping}
plan(multisession, workers = parallel::detectCores() - 1) 
base <- "https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_"
htmls <- paste0(base, "page_", 1:10, ".html")

scrape_table <- function(url) {
  web_page <- read_html(url)
  table <- web_page %>%
    html_nodes("table.table-striped") %>%
    html_table(fill = TRUE)
  return(table[[1]])  # Extraer el primer elemento (que es la tabla real)
}
tables_list <- future_map(htmls, scrape_table)
db <- bind_rows(tables_list) %>% select(-...1)

plan(sequential) 
    db = bind_rows(tables_list) %>% select(-...1)
    rm(web_page,table,base,htmls,url,scrape_table)
```

</div>

### 2.c) Data cleaning process
#### 2.c.1) FRAMMING MISSING VALUES

```{r Cleaning of data set, echo=FALSE}
#--------------------FILTERING BY +18, EMPLOYED IN BOGOTÁ-----------------------

db2 <- db %>% filter(age > 18, ocu == 1, dominio == "BOGOTA" )

#------------ FRAMING MISSING VALUES -------------------------------------------
Nobs <- nrow(db2) 
db_miss <- data.frame(
  Variable = names(db2),
  n_missing = colSums(is.na(db2)),
  p_missing = round(colSums(100*is.na(db2))/Nobs,2)
) 
rownames(db_miss) <- NULL

question <- read_html("https://ignaciomsarmiento.github.io/GEIH2018_sample/dictionary.html")
question <- question %>%
        html_nodes("table") %>%
        html_table(fill = TRUE)
question = bind_rows(question)
db_miss <- left_join(question, db_miss, by = "Variable")
db_miss <- db_miss %>% arrange(desc(p_missing)) %>% filter(p_missing != 0) 
write.csv(db_miss, "1. Missing values per variable.csv", row.names = FALSE)

# SELECTING RELEVANT VARIABLES
db2 <- db2 %>%
  select(y_ingLab_m, y_salary_m, y_total_m, ie,iees,  
         ingtotes, ingtotob, ingtot, y_bonificaciones_m, y_gananciaNeta_m, 
         y_gananciaNetaAgro_m, y_primaNavidad_m, y_primaServicios_m, y_primaVacaciones_m, 
         maxEducLevel, directorio, secuencia_p, orden, clase, dominio, mes, estrato1, 
         sex, age, oficio, fex_c, depto, fex_dpto, fweight, 
         informal,p6240,ocu,formal,informal,iees, imdi, impa, iof1, iof2, iof3h, iof6, isa,
         iees, imdies, impaes,  iof1es, iof2es, iof3hes, iof3ies, iof6es, isaes,
         cclasnr4, cclasnr5, cclasnr2, cclasnr6, cclasnr7, cclasnr8,
         cclasnr11, cclasnr3,y_otros_m,y_vivienda_m,y_gananciaIndep_m)
#skim(db2)
```

#### 2.c.2) CHOOSING VARIABLES

```{r Data cleaning, echo=FALSE, message=FALSE, warning=FALSE}
#------------ DATA CLEANING PROCESS: CONSISTENCY -------------------------------
# P6240 to text for tables.
db2 <- db2 %>% mutate(Occupation = if_else(is.na(p6240), NA,
                              case_when(
                                p6240 == 6 ~ "Other activity",
                                p6240 == 5 ~ "Permanently unable to work",
                                p6240 == 4 ~ "Domestic work",
                                p6240 == 3 ~ "Studying",
                                p6240 == 2 ~ "Looking for work",
                                p6240 == 1 ~ "Working",
                                TRUE ~ as.character(p6240)
                              )))

# OCCUPIED = FORMAL + INFORMAL. 
Occupied_description <- db2 %>%
  group_by(Occupation) %>%
  summarise(across(c(ocu, formal, informal), sum, na.rm = TRUE)) %>%
  ungroup() %>%
 add_row(Occupation = "Total", 
          ocu = sum(.$ocu), 
          formal = sum(.$formal), 
          informal = sum(.$informal)) %>%
    select(Occupation, ocu, formal, informal) %>%
  mutate(across(c(ocu, formal, informal), ~ . / last(ocu)*100))

Occupied_description <- Occupied_description[-c(7),]

Occupied_description <- Occupied_description %>%
  mutate(across(c(ocu, formal, informal), ~ sprintf("%.1f", .x)))
stargazer(Occupied_description, summary = FALSE, type = "latex", rownames = FALSE,
          out = "2.1.Occupied_description_table.txt")

#------------ DATA CLEANING PROCESS: EXPLORING INCOME --------------------------

#-------------------OBSERVED vs IMPUTED VARIABLES-------------------------------
obs_vars <- c("ie",  "impa", "ingtot", "iof1", "iof2", "iof3h", "iof3i", "iof6", "isa")
imp_vars <- c("iees", "impaes", "ingtotes", "iof1es", "iof2es", "iof3hes", "iof3ies", "iof6es", "isaes")
titles   <- c("Especie", "Main Activity", "Total","Interes and dividends", "Retirement", "Household Aid", "Institutional Aid", 
              "Real State", "Second Activity")

# ----> PROOF OF: IMPUTED VARIABLES COVER SOME MISSING VALUES OF OBSERVED VARIABLES
imp_vars2 <- c("iees", "imdies", "impaes", "iof1es", "iof2es", "iof3hes", "iof3ies", "iof6es", "isaes")
missing_report <- c("cclasnr4", "cclasnr5", "cclasnr2", "cclasnr6", "cclasnr7", "cclasnr8", "cclasnr8", "cclasnr11", "cclasnr3")

check <- function(db, imp_var, missing_var) {
  nrow(db2 %>% filter(!is.na(!!sym(imp_var)), !!sym(missing_var) != 1)) == 0} 
  # MISSING VALUES OF OBSERVED VALUES ARE COVERED BY IMPUTED VALUES? RETURNS TRUE IF CONDITION IS MET
mapply(function(imp, miss) check(db2, imp, miss), imp_vars2, missing_report); rm(imp_vars2,missing_report)

# ---> FILLING EXTREME MISSING VALUES WITH IMPUTED VALUES
for(i in seq_along(obs_vars)) {
  new_name <- paste0(obs_vars[i], "_oi")
  # Convert NULL to NA
  imp_val <- if (!is.null(db2[[imp_vars[i]]])) db2[[imp_vars[i]]] else rep(NA, nrow(db2))
  obs_val <- if (!is.null(db2[[obs_vars[i]]])) db2[[obs_vars[i]]] else rep(NA, nrow(db2))
  db2[[new_name]] <- ifelse(!is.na(imp_val), imp_val, obs_val)
}

#-------------- "Y" CONSTRUCTED VARIABLES VS OBSERVES+IMPUTED CORRECTIONS-------
# -----> CHOOSING BETWEEEN MANUEL'S/ IGNACIO VARIABLES AND OBSERVERD+IMPUTED

#  CORRELATION PLOT
db_temp <- db2 %>%
  mutate(
    Other = ie_oi,
    `Total Income` = ingtot_oi,
    `Informal Salary` = ifelse(informal == 1, impa_oi, NA),
    `Formal Salary` = ifelse(formal == 1, impa_oi, NA),
    `Domestic Work` = ifelse(p6240 == 4, impa_oi, NA)
  ) %>%
  select( Other, `Informal Salary`, `Formal Salary`,`Total Income`,`Domestic Work`,
          `Y. Others` = y_otros_m,`Y. Informal Salary` = y_gananciaNeta_m,`Y. Formal Salary` = y_ingLab_m,
          `Y. Total Income` = y_total_m,`Y. Domestic Work` = y_vivienda_m,
          )

db_temp <- db_temp %>% 
  mutate_all(~ ifelse(is.na(.) | . == 0, 0, 1))
db_temp <-  db_temp %>%  select(which(apply(db_temp, 2, sd) > 0))

M <- cor(db_temp, use = "pairwise.complete.obs")
corrplot(
  M, method = "color",         # Use colored squares
  col = colorRampPalette(c("lightgray", "white", "blue"))(200),  
  tl.col = "black", tl.srt = 90,   tl.cex = 0.8, cl.cex = 0.9,                
  addCoef.col = "black", number.cex = 0.8,mar = c(2,2,2,2))

# ----> CHOOSING SALARY VARIABLES (WORKING AS FIRST ACTIVITY)
db_temp <-  db2 %>% filter(p6240 == 1) %>%  select(y_salary_m, impa_oi)
summary(db_temp)
sapply(db_temp, function(col) sum(col == 0, na.rm = TRUE)) # Number of 0 values
sapply(db_temp, function(col) {
  if (is.numeric(col)) { cut(col, breaks = quantile(col, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE), include.lowest = TRUE) %>%
      table()
  } else {
    NA # OBSERVATIONS PER QUANTILE
  }
})

# ----> CHOOSING TOTAL INCOME
TOTAL_INCOME <- data.frame(
  Metric = c("Mean", "NA Count", "Zero Count", "Q1", "Median", "Q3"),
  
  ingtot_oi = sapply(list(
    function(x) mean(x, na.rm = TRUE), 
    function(x) sum(is.na(x)), 
    function(x) sum(x == 0, na.rm = TRUE),
    function(x) quantile(x, 0.25, na.rm = TRUE), 
    function(x) median(x, na.rm = TRUE), 
    function(x) quantile(x, 0.75, na.rm = TRUE)
  ), function(f) f(db2$ingtot_oi)),
  
  y_total_m = sapply(list(
    function(x) mean(x, na.rm = TRUE), 
    function(x) sum(is.na(x)), 
    function(x) sum(x == 0, na.rm = TRUE),
    function(x) quantile(x, 0.25, na.rm = TRUE), 
    function(x) median(x, na.rm = TRUE), 
    function(x) quantile(x, 0.75, na.rm = TRUE)
  ), function(f) f(db2$y_total_m))
)

TOTAL_INCOME$Difference <- with(TOTAL_INCOME, ingtot_oi - y_total_m)
stargazer(TOTAL_INCOME, summary = FALSE, type = "latex",title = "Total Income variables comparison",
          label = "tab:total_income",out = "2.1 TOTAL INCOME.tex")

#------------------ EXTRA VALIDATION -------------------------------------------
# ----> CHECKING DISPERSION OF OBSERVED VS IMPUTED 
Stats <- mapply(function(ob, im, title) {
  obs <- db2[[ob]]; imp <- db2[[im]]
  q_obs <- quantile(obs, c(0.05, 0.25, 0.5, 0.75, 0.95), na.rm = TRUE)
  q_imp <- quantile(imp, c(0.05, 0.25, 0.5, 0.75, 0.95), na.rm = TRUE)
  
  data.frame(
    Statistic = c("Mean", "SD", "NA values", paste0("Quantile_", c(5, 25, 50, 75, 95))),
    Obs = c(mean(obs, na.rm = TRUE), sd(obs, na.rm = TRUE), sum(is.na(obs)), round(q_obs, 2)),
    Imp = c(mean(imp, na.rm = TRUE), sd(imp, na.rm = TRUE), sum(is.na(imp)), round(q_imp, 2))
  ) %>% rename_with(~ paste0(title, "_", sub("^(Obs|Imp)$", "\\1", .x)), Obs:Imp)
}, obs_vars, imp_vars, titles, SIMPLIFY = FALSE)
Obs_vs_Imp <- Reduce(function(x, y) merge(x, y, by = "Statistic", sort = FALSE), Stats)

# ---> DIFFERENCES OBSERVED VS IMPUTED
 cols <- 2:ncol(Obs_vs_Imp)
 obs_idx <- cols[seq(1, length(cols), by = 2)]
 imp_idx <- cols[seq(2, length(cols), by = 2)]
 Differences <- Obs_vs_Imp[, imp_idx] - Obs_vs_Imp[, obs_idx]
 colnames(Differences) <- titles
 Differences <- cbind(Statistic = Obs_vs_Imp[, 1], Differences)
 Differences <- Differences[-c(9),] 
 stargazer(Differences, summary = FALSE, type = "latex", rownames = FALSE,
           out = "2.2.Differences Observed vs Imputed.txt")

# ----> OJALÁ SALGA
# -----> EVALUATING DENSITY OF OBSERVED VS OBSERVED +IMPUTED
# obs_vars <- c("ie",  "impa", "ingtot", "iof1", "iof6", "isa")
# obs_imp <- c("ie_oi", "impa_oi", "ingtot_oi", "iof6_oi", "isa_oi")
# titles   <- c("Especie", "Main Activity", "Total","Interes and dividends","Real State", "Second Activity")
# 
# par(mfrow = c(2, 5), oma = c(0, 0, 5, 0))  
#  for(i in seq_along(obs_vars)) {
#    # Extract observed and imputado data
#    data_obs <- db2[[obs_vars[i]]]
#    data_imp <- db2[[obs_imp[i]]]
#    q_obs <- quantile(data_obs, probs = c(0.05, 0.95), na.rm = TRUE) # Exclude extreme percentiles
#    data_obs <- data_obs[data_obs >= q_obs[1] & data_obs <= q_obs[2]] 
#    q_imp <- quantile(data_imp, probs = c(0.05, 0.95), na.rm = TRUE)
#    data_imp <- data_imp[data_imp >= q_imp[1] & data_imp <= q_imp[2]]
#    dens_obs <- density(data_obs, na.rm = TRUE) # Compute density
#    dens_imp <- density(data_imp, na.rm = TRUE)
#    
#    xlim <- range(c(dens_obs$x, dens_imp$x), na.rm = TRUE)
#    ylim <- range(c(dens_obs$y, dens_imp$y), na.rm = TRUE)
#    
#    plot(dens_obs, xlim = xlim, ylim = ylim,
#         main = titles[i], xlab = "", ylab = "Density",
#         col = "black", lwd = 2, type = "l")
#    
#    lines(dens_imp, col = "red", lwd = 2)
#  }
#  
#  par(fig = c(0, 1, 0.92, 1), new = TRUE)
#  plot(0, type = "n", axes = FALSE, xlab = "", ylab = "")
#  legend("center", legend = c("Observed", "Imputed"),
#         col = c("black", "red"), lty = 1, horiz = TRUE, bty = "n", cex = 0.8)

```

### 2.d) FILLING NA'S

```{r Filling NA's, echo=FALSE, message=FALSE, warning=FALSE}
# ----------------------DATA STATISTICAL DESCRIPTION ---------------------------
db2 <- db2 %>% mutate(Sexo = case_when(
                                sex == 0 ~ "Women",
                                sex == 1 ~ "Men" ))
db2 <- db2 %>% mutate(Sexo = case_when(
                                sex == 0 ~ "Women",
                                sex == 1 ~ "Men" ))

# ----> SALARY INCOME: SPOTTING % OF NA's to FILL 
WAGES <- db2 %>%
  group_by(Occupation, Sexo) %>%
  summarise(
    mean_age = mean(age, na.rm = TRUE),
    Q1_wages = mean(impa_oi[impa_oi <= quantile(impa_oi, 0.25, na.rm = TRUE)], na.rm = TRUE),
    mean_wages = mean(impa_oi, na.rm = TRUE),
    Q3_wages = mean(impa_oi[impa_oi >= quantile(impa_oi, 0.75, na.rm = TRUE)], na.rm = TRUE),
    na_or_zero_percentage_total = sum(is.na(impa_oi) | impa_oi <= 10000) / n() * 100
  ) %>%
  ungroup() %>%
  mutate(across(where(is.numeric), ~ round(.x, 0)))

stargazer(WAGES, type = "latex", summary = FALSE, title = "Wage description by Occupation and Sex",
          label = "tab:WAGES", out = "2.1 WAGES.tex",
          digits = 1)

# ------------------------- FILLING NA's VALUES ---------------------------------
# ---> MAX LEVEL EDU: ONLY 10% MISSING. FILL WITH MODE
mode_edu <- as.numeric(names(sort(table(db2$maxEducLevel), decreasing = TRUE)[1]))
db2 <- db2  %>%
  mutate(maxEducLevel = ifelse(is.na(maxEducLevel) == TRUE, mode_edu , maxEducLevel))

# ----> FILLING SALARY VARIABLE. 
db2 <- db2 %>% mutate(working = ifelse(p6240 == 1, 1, 0))

db2$maxEducLevel<- factor(db2$maxEducLevel)
dummy_maxEducLevel <- as.data.frame(model.matrix(~ maxEducLevel - 1, data = db2))
db2 <- cbind(db2, dummy_maxEducLevel)

lM1 <- lm(impa_oi ~ formal + working + maxEducLevel, data = db2)
summary(lM1)

# ----> EVALUATING LEVERAGE
db2$leverage <- NA
db2$leverage[as.numeric(names(hatvalues(lM1)))] <- hatvalues(lM1)

db2$residuals <- NA
db2$residuals[as.numeric(names(residuals(lM1)))] <- residuals(lM1)

N <- nrow(db2)
db2$id<- seq(1 , N)

LEV <- ggplot(db2 , aes(y = leverage , x = id , color= Occupation )) +
  geom_point() + 
  theme_bw() + 
  labs(x = "Observations",  
       y = "Leverage",
       title = "")

RES <- ggplot(db2 , aes(y = leverage , x = residuals, color= Occupation )) +
  geom_point() + # add points
  theme_bw() + #black and white theme
  labs(x = "Residuals",  
       y = "Leverage",
       title = "") # labels

grid.arrange(LEV, RES, ncol = 2)

# CUTTING HIGH LEVERAGE OBSERVATIONS
p <- mean(db2$leverage, na.rm = TRUE)
p
cutt <- 3*p

db2 <-  db2 %>% 
  dplyr:: filter(leverage<= cutt)

# RE RUNNING THE MODEL
lM2 <- lm(impa_oi ~ formal + working + maxEducLevel, data = db2)
summary(lM2)
db2$impa_oi_P <- predict(lM2, newdata = db2)
db2$impa_oi <- ifelse(is.na(db2$impa_oi) | db2$impa_oi == 0, db2$impa_oi_P, db2$impa_oi)

# ----> CHECKING FOR NEGATIVE VALUES
non_positive_impa_oi <- sum(db2$impa_oi <= 0)
cat("Number of non-positive values in y_salary_m:", non_positive_impa_oi, "\n")

# ----> CHECKING RESIDUALS
png("2.3 Residuals removing HIGH LEVERAGE.png", width = 800, height = 600) 
par(mfrow = c(2, 2))
plot(lM2)
dev.off()

rm(db_miss,question, M,Nobs,mode_edu,dummy_maxEducLevel,lM1, Occupied_description,imp_vars,obs_vars,titles,Stats,Obs_vs_Imp, imp_idx,obs_idx,cols, check,lM2,N,WAGES,TOTAL_INCOME,RES,LEV, Differences, db_temp, tables_list, cutt, i, imp_val, new_name, obs_val, p)
```


## 3. Age wage profile
### 3.1 Running the regression

```{r Age wage Regression, echo=FALSE, warning=FALSE}
# ------------ RUNNING THE REGRESSION ------------------------------------------
model_salary <- lm(log(impa_oi) ~ age + I(age^2), data = db2)
summary(model_salary)

# ------------ EVALUATING RESIDUALS --------------------------------------------
png("3.1 Salary vs Age.png", width = 800, height = 600) 
par(mfrow = c(2, 2))
plot(model_salary)
dev.off()

# Is age^2 worth it? Yes it is jeje 
model_salary2 <- lm(log(y_salary_m) ~ age, data = db2)
anova(model_salary , model_salary2)

# ------------BOOTSTRAPPING RESULTS --------------------------------------------
set.seed(100)  # For reproducibility
boot_model_salary = do(10000)*lm(log(impa_oi) ~ age + I(age^2),data=mosaic::resample(db2))

boot_summary <- boot_model_salary %>%
  summarise(
    age = mean(age),
    age_se = sd(age),
    age_lower = quantile(age, 0.025),
    age_upper = quantile(age, 0.975),
    age2 = mean(I(age^2)),
    age2_se = sd(I(age^2)),
    age2_lower = quantile(I(age^2), 0.025),
    age2_upper = quantile(I(age^2), 0.975)
  )

model_function <- function(data, indices) {
  sample_data <- data[indices, ]
  model <- lm(log(impa_oi) ~ age + I(age^2), data = sample_data)
  return(coef(model)) 
}

boot_model_salary2 <- boot(db2, model_function, R = 1000)

# BOOTSTRAP STANDAR ERRORS
boot_se <- apply(boot_model_salary2$t, 2, sd)

# ------------ EXPORTING TO LATEX R --------------------------------------------
stargazer( model_salary, model_salary,
           type = "latex",se = list(summary(model_salary)$coefficients[, 2], boot_se), 
           title = "Monthly Income Salary Relation with Age",
           covariate.labels = c("Intercept", "Age", "Age Squared"),
           dep.var.labels = "Labor Monthly Income*",
           column.labels = c("MCO", "Bootstrap"),
           star.cutoffs = c(0.1, 0.05, 0.01),
           ci = TRUE, 
           ci.level = 0.95,
           out = "Salary and Age.tex",
           notes = c("Statistical significance levels: *** p<0.01, ** p<0.05, * p<0.1."),
           add.lines = list(c("Method", "MCO", "Bootstrap")
  )
)

# ------------ BOOTSTRAPPING FOR PEAK AGE --------------------------------------
peak_age_f <- function(data, indices) {
  sample_data <- data[indices, ]
  model <- lm(log(impa_oi) ~ age + I(age^2), data = sample_data)
  coef_model <- coef(model)
  peak_age <- -coef_model["age"] / (2 * coef_model["I(age^2)"])
  return(peak_age)
}

set.seed(100)
boot_peak_age <- boot(db2, peak_age_f, R = 1000)
# Calculate confidence intervals for the peak age
peak_age_ci <- boot.ci(boot_peak_age, type = "perc")
print(peak_age_ci)

# ------------ PLOTTING THE AGE-EARNINGS PROFILE --------------------------
# Define a single function to fit the model and calculate peak age
peak_age_f <- function(data, indices) {
  sample_data <- data[indices, ]
  model <- lm(log(impa_oi) ~ age + I(age^2), data = sample_data)
  coef_model <- coef(model)
  peak_age <- -coef_model["age"] / (2 * coef_model["I(age^2)"])
  return(peak_age)
}

# Run the bootstrap with 1000 samples
set.seed(100)
boot_peak_age <- boot(db2, peak_age_f, R = 1000)

# Calculate confidence intervals for the peak age
peak_age_ci <- boot.ci(boot_peak_age, type = "perc")
print(peak_age_ci)

# Calculate the original peak age using the full dataset
original_peak_age <- peak_age_f(db2, 1:nrow(db2))

cat("Original Peak Age:", round(original_peak_age, 2), "\n")
cat("95% Bootstrap CI for Peak Age:", 
    round(peak_age_ci$percent[4], 2), "to", 
    round(peak_age_ci$percent[5], 2), "\n")

# Fit the quadratic model
model_salary <- lm(log(impa_oi) ~ age + I(age^2), data = db2)

# Create a data frame for prediction
age_values <- seq(min(db2$age), max(db2$age), by = 1)
predicted_earnings <- predict(model_salary, newdata = data.frame(age = age_values))

# Create a data frame for ggplot
plot_data <- data.frame(age = age_values, log_salary = predicted_earnings)

# Plot the age-earnings profile with the peak age and its confidence interval
ggplot(plot_data, aes(x = age, y = log_salary)) +
  geom_line(color = "blue", size = 1) +
  geom_vline(xintercept = original_peak_age, color = "red", linetype = "dashed") +
  geom_vline(xintercept = peak_age_ci$percent[4], color = "green", linetype = "dotted") +
  geom_vline(xintercept = peak_age_ci$percent[5], color = "green", linetype = "dotted") +
  labs(title = "Estimated Age-Earnings Profile",
       x = "Age",
       y = "Log(Monthly Income Salary)",
       subtitle = paste("Peak Age:", round(original_peak_age, 2), 
                        "95% CI [", 
                        round(peak_age_ci$percent[4], 2), ",", 
                        round(peak_age_ci$percent[5], 2), "]")) +
  theme_minimal()
```

### 3.1 Evaluating residuals

Análisis preliminar de las regresiones:

-   Residuals vs Fitted: Podemos ver que en general los residuales se
    concentran etre -4 y 4. Que no parece existir una tendencia muy
    literal, aunque si la existiera sería negativa. En general los
    valores se concentran entre 13.5 y 14.0 es decir que el valor
    predicho de log(w) suele estar entre este rango para casi todas las
    observaciones, aunque con notables outliers para algunas
    observaciones. ¿Eso qué quiere decir? Que un cambio de 1 año de experiencia y su variación (cómo explico la experiencia al cuadrado?) genera un crecimiento (creo aproximado de 442.413 COP (COP?) 

-   Q-Q Plot: Claramente hay heterocedasticidad, presencia de colas
    psadas que nos indicanque existe una distribución asimetrica de los errores en 
    los cuantiles más extremos.
-   Scale-Location: Nuevamente sugiere que la varianza no es constante
-   Residuals vs Leverage: Nos muestra que existen observaciones con gran influencia. Tal cómo la 9452, 943, 9453 que pueden estar moviendo esa pendiente a niveles distorsionados.


```{r Evaluating residuals, echo=FALSE}

```

## 4. The gender earnings GAP
```{r Unconditional Gender Wage Gap}

# We are transforming the 'sex' variable to create a new 'Female' indicator where
#female = 1 for females (sex = 0) and Female = 0 for males (sex = 1) (According to data dictionary)

db2 <- db2 %>%
  mutate(female = ifelse(sex == 0, 1, 0)) 
head(db2)

# Checking and removing non-positive values in y_salary_m
non_positive_y_salary_m <- sum(db2$y_salary_m <= 0)
cat("Number of non-positive values in y_salary_m:", non_positive_y_salary_m, "\n")
db2 <- db2 %>% filter(y_salary_m > 0)

model_unconditional <- lm(log(y_salary_m) ~ female, data = db2)
summary(model_unconditional)

#  GAUSS MARCOV THEOREM 
#  Linearity Check:
ggplot(data = data.frame(Fitted = fitted(model_unconditional), Residuals = residuals(model_unconditional)), 
       aes(x = Fitted, y = Residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs Fitted Values", x = "Fitted Values", y = "Residuals")

# Exogeneity: Mean of residuals should be zero
mean(residuals(model_unconditional))

# Homoscedasticity: Breusch-Pagan Test
bptest(model_unconditional)  #P-value < 0.05 suggests rejecting the null hypothesis

# No Autocorrelation: Durbin-Watson Test
dwtest(model_unconditional) # P-value < 0.05 suggests rejecting the null hypothesis
```
The unconditional model provides clear evidence of a gender wage gap, with females earning significantly less than males on average, as indicated by the highly statistically significant coefficient for the gender variable female. This suggests that gender plays a significant role in explaining variations in log-transformed salaries, highlighting potential gender disparities in earnings,however, the statistical significance of the model must be interpreted with caution due to violations of key assumptions: The extremely low p-values from the Breusch-Pagan test for heteroscedasticity and the Durbin-Watson test for autocorrelation reveal strong evidence against the null hypotheses, indicating issues with homoscedasticity and the independence of errors. These violations imply that traditional inference methods may be flawed, potentially affecting the reliability of the standard errors and p-values. To draw more robust conclusions, we proceed with estimating a conditional model that includes control variables and addresses the issues of heteroscedasticity and autocorrelation. 

```{r Conditional Gender Wage Gap}

# Conditional Gender Wage Gap (OLS)
conditional_model <- lm(log(y_salary_m) ~ female + age + I(age^2) + maxEducLevel + informal + estrato1, data = db2)
summary(conditional_model)

# ------------------------ FRISCH-WAUGH-LOVELL THEOREM--------------------------------
#Estimation with FWL to isolate the effect of female on log(salary) by partialling out the control variables. 
# Fit the control model (without female)
controls <- lm(log(y_salary_m) ~ age + I(age^2) + maxEducLevel + informal + estrato1, data = db2)
residuals_w <- residuals(controls)

gender <- lm(female ~ age + I(age^2) + maxEducLevel + informal + estrato1, data = db2)
residuals_x <- residuals(gender)

#The effect of female using the residuals
fwl_model_1 <- lm(residuals_w ~ residuals_x)
summary_fwl <- summary(fwl_model_1)  
summary_fwl

#  GAUSS MARCOV THEOREM 
# 1. Linearity Check: Residual vs Fitted Plot
ggplot(data = data.frame(Fitted = fitted(model_unconditional), Residuals = residuals(fwl_model_1)), 
       aes(x = Fitted, y = Residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs Fitted Values", x = "Fitted Values", y = "Residuals")

# 2. Exogeneity: Mean of residuals should be zero
mean(residuals(fwl_model_1)) # Mean is aprox. 0 supporting the assumption of exogeneity

# 3. Homoscedasticity: Breusch-Pagan Test
bptest(fwl_model_1)  #P-value > 0.05 suggests no strong evidence of heteroscedasticity in the residuals

# 4. No Autocorrelation: Durbin-Watson Test
dwtest(fwl_model_1) # #P-value < 0.05 suggests rejecting the null hypothesis

# 5. Multicollinearity
vif_values <- vif(conditional_model)
vif_values #Multicollinearity is moderate to high for age and I(age^2) (GVIF^(1/(2*Df)) > 5), which is expected because I(age^2) is a transformation of age. The other variables do not exhibit multicollinearity.

robust_se <- coeftest(fwl_model_1, vcov = vcovHC(fwl_model_1, type = "HC1"))
print(robust_se)

#The model shows a highly significant negative effect of being female on salary,
#with a coefficient of -0.217. The standard error for this coefficient is small (0.0113),
#indicating precise estimation, the intercept is not significant, which is expected since
#residuals are used and finally the robust standard errors suggest the model has effectively addressed 
#autocorrelation issues, providing solid and reliable results.

# ------------------------ FWL WITH BOOTSTRAP ----------------------------------------
# Función de bootstrap para FWL
# Función de bootstrap para FWL
fwl_bootstrap <- function(data, indices) {
  # Resample the data
  boot_data <- data[indices, ]
  control_model_boot <- lm(log(y_salary_m) ~ age + maxEducLevel + informal + estrato1, data = boot_data)
  boot_data$residuals_control <- residuals(control_model_boot)
  fwl_model_boot <- lm(residuals_control ~ female, data = boot_data)
  return(c(coef(fwl_model_boot)["female"]))
}

# Bootstrap with 1000 resamples
set.seed(100)  # For reproducibility
fwl_boot_results <- boot(data = db2, statistic = fwl_bootstrap, R = 1000)

# Ajustar el modelo fuera de la función de bootstrap para pruebas y gráficos
control_model <- lm(log(y_salary_m) ~ age + maxEducLevel + informal + estrato1, data = db2)
db2$residuals_control <- residuals(control_model)
fwl_boot_model <- lm(residuals_control ~ female, data = db2)
summary(fwl_boot_model)

#  GAUSS MARKOV THEOREM 
# 1. Linearity Check: Residual vs Fitted Plot
ggplot(data = data.frame(Fitted = fitted(fwl_boot_model), Residuals = residuals(fwl_boot_model)), 
       aes(x = Fitted, y = Residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs Fitted Values", x = "Fitted Values", y = "Residuals")

# 2. Exogeneity: Mean of residuals should be zero
mean_residuals <- mean(residuals(fwl_boot_model)) # Mean is approx. 0 supporting the assumption of exogeneity
print(mean_residuals)

# 3. Homoscedasticity: Breusch-Pagan Test
bp_test <- bptest(fwl_boot_model)  # P-value < 0.05 suggests rejecting the null hypothesis, and evidence of hero
print(bp_test)

# 4. No Autocorrelation: Durbin-Watson Test
dw_test <- dwtest(fwl_boot_model) # P-value < 0.05 suggests rejecting the null hypothesis
print(dw_test)

# Comparando los resultados
boot_se <- sd(boot_estimates)
boot_mean <- mean(boot_estimates)

results <- data.frame(
  Method = c("FWL", "Bootstrap"),
  Estimate = c(coef(fwl_boot_model)["female"], boot_mean),
  StdError = c(summary(fwl_boot_model)$coefficients["female", "Std. Error"], boot_se)
)
print(results)

coef_names <- names(coef(fwl_model_1))
coef_names[2] <- "female"

# Comparing the results
stargazer(
  model_unconditional, conditional_model, fwl_model_1,
  title = "Comparison of Unconditional and Conditional Gender Wage Gaps",
  type = "text", # Cambia a "html" o "latex" para otros formatos
  align = TRUE, 
  out = "gender_wage_gap_comparison.html",
  covariate.labels = c("Female"), # Solo mostramos la etiqueta para "Female"
  column.labels = c("Unconditional", "Conditional", "FWL"),
  dep.var.labels = "Log(Salary)",
  keep = c("female", "residuals_x"), # Solo mantenemos "female" y "residuals_x"
  no.space = TRUE,
  add.lines = list(
    c("Method", "Unconditional", "Conditional", "FWL"),
    c("Observations", nobs(model_unconditional), nobs(conditional_model), nobs(fwl_model_1))
  ),
  notes = "Standard errors are in parentheses."
)
```

```{r Peak ages}

# Regression model with age-gender interaction
peak_ages_model <- lm(log(y_salary_m) ~ age * female + I(age^2) * female + maxEducLevel + informal + estrato1, data = db2)

# Create a range of ages and genders for prediction
age_seq <- seq(min(db2$age), max(db2$age), by = 1)
prediction_data <- expand.grid(age = age_seq, female = c(0, 1))

# Assign typical values for other variables
common_educ <- names(sort(table(db2$maxEducLevel), decreasing = TRUE))[1]  # Mode of education
common_informal <- as.numeric(names(sort(table(db2$informal), decreasing = TRUE))[1])  # Mode of informality
common_estrato <- as.numeric(names(sort(table(db2$estrato1), decreasing = TRUE))[1])  # Mode of socioeconomic level

# Add fixed values to prediction_data
prediction_data$maxEducLevel <- factor(common_educ, levels = levels(db2$maxEducLevel))
prediction_data$informal <- common_informal
prediction_data$estrato1 <- common_estrato

# Make predictions with confidence intervals
predictions <- predict(peak_ages_model, newdata = prediction_data, interval = "confidence")
prediction_data <- cbind(prediction_data, predictions)

# Back-transform log wages to actual wages for plotting
prediction_data <- prediction_data %>%
  mutate(pred_wage = exp(fit), 
         lwr_wage = exp(lwr), 
         upr_wage = exp(upr))


# Plot predicted age-wage profile by gender
ggplot(prediction_data, aes(x = age, y = pred_wage, color = as.factor(female), fill = as.factor(female))) +
  geom_line(linewidth = 1) +  # Use linewidth instead of size
  geom_ribbon(aes(ymin = lwr_wage, ymax = upr_wage), alpha = 0.2) +
  labs(title = "Predicted Age-Wage Profile by Gender",
       subtitle = "Includes confidence intervals based on regression estimates",
       x = "Age",
       y = "Predicted Wage",
       color = "Gender",
       fill = "Gender",
       caption = "Note: The predicted age-wage profile is estimated using a regression model with interaction terms.\nConfidence intervals reflect the uncertainty of predictions.\nSource: Own estimations based on regression model.") +
  scale_color_manual(values = c("blue", "pink"), labels = c("Male", "Female")) +
  scale_fill_manual(values = c("blue", "pink"), labels = c("Male", "Female")) +
  theme_minimal() +
  theme(plot.caption = element_text(hjust = 0, size = 10))  # Left-align caption for readability

# Extract coefficients from the model
coefficients <- coef(peak_ages_model)
age_coef <- coefficients["age"]
age_squared_coef <- coefficients["I(age^2)"]
female_age_coef <- coefficients["age:female"]
female_age_squared_coef <- coefficients["female:I(age^2)"]

# Peak ages
if (all(!is.na(c(age_coef, age_squared_coef, female_age_coef, female_age_squared_coef)))) {
  peak_age_male <- round(-age_coef / (2 * age_squared_coef))
  peak_age_female <- round(-(age_coef + female_age_coef) / (2 * (age_squared_coef + female_age_squared_coef)))
  
  cat("Peak age for males:", peak_age_male, "\n")
  cat("Peak age for females:", peak_age_female, "\n")
} else {
  cat("Missing coefficients for calculation.\n")
}

```
The statistical significance of the model is evident from the extremely low p-values associated with both the Breusch-Pagan test for heteroscedasticity and the Durbin-Watson test for autocorrelation, these results indicate strong evidence against the null hypotheses, revealing violations in the homoscedasticity and independence of errors assumptions, respectively. Economically, the model's findings suggest that the gender variable (female) plays a significant role in explaining variations in the log-transformed salary, highlighting potential gender disparities in earnings, however, the presence of heteroscedasticity and autocorrelation implies that traditional inference may be flawed, necessitating robust econometric techniques to ensure the validity of the estimated effects.

## 5. Predicting earnings

Split the sample into two: a training (70%) and a testing (30%) sample. (Don’t
forget to set a seed to achieve reproducibility. In R, for example you can use
set.seed(10101), where 10101 is the seed.)

```{r}
set.seed(1541)

db2 = db2 %>%
  mutate(oficio2 = case_when(
    oficio %in% c(1, 2, 9, 12) ~ "Profesionales Altamente Calificados",
    oficio %in% c(3, 5, 7, 8) ~ "Técnicos y Tecnólogos",
    oficio %in% c(11, 31, 32, 33, 39) ~ "Administración y Oficinas",
    oficio %in% c(41, 43, 45) ~ "Ventas y Comerciantes",
    oficio %in% c(16, 17, 15) ~ "Arte y Cultura",
    oficio %in% c(54, 57, 58) ~ "Servicios Personales",
    oficio %in% c(98, 97, 36, 37) ~ "Transporte y Logística",
    oficio %in% c(61, 62, 71) ~ "Agricultura y Minería",
    oficio %in% c(81, 83, 85, 95) ~ "Oficios Industriales",
    oficio %in% c(21, 40) ~ "Directivos y Gerentes",
    TRUE ~ "Otros"
  ))

data = db2 |> 
  drop_na(y_salary_m)

in_train = createDataPartition(y = data$y_salary_m, 
                               p = 0.7, 
                               list = FALSE)
training = data %>% 
  filter(row_number() %in% in_train)

test = data %>% 
  filter(!row_number() %in% in_train)

for (var in c("maxEducLevel", "estrato1", "oficio2")) {
  training[[var]] <- as.factor(training[[var]])
  test[[var]] <- factor(test[[var]], levels = levels(training[[var]]))
}

```

Report and compare the predictive performance in terms of the RMSE of all
the previous specifications with at least five (5) additional specifications that
explore non-linearities and complexity.

```{r}
formula_1 = formula(log(y_salary_m) ~ age + I(age^2))
formula_2 = formula(log(y_salary_m) ~ female)
formula_3 = formula(log(y_salary_m) ~ female + age + I(age^2) + factor(maxEducLevel) + informal + factor(estrato1))

formula_4 = formula(log(y_salary_m) ~ female + age + I(age^2) + factor(maxEducLevel) + informal + factor(estrato1) + factor(oficio2))

formula_5 = formula(log(y_salary_m) ~ female * factor(maxEducLevel) + age + I(age^2) + informal + factor(estrato1) + factor(oficio2))

formula_6 = formula(log(y_salary_m) ~ female * factor(maxEducLevel) + poly(age, 3) + informal * factor(maxEducLevel) + factor(estrato1) + factor(oficio2))

formula_7 = formula(log(y_salary_m) ~ female * factor(maxEducLevel) + poly(age, 3) + informal * factor(maxEducLevel) + factor(estrato1) * factor(maxEducLevel) + factor(oficio2))

formula_8 = formula(log(y_salary_m) ~ female * factor(maxEducLevel) * factor(estrato1) + poly(age, 3) + informal * factor(maxEducLevel) + factor(oficio2))

```

Calculate the predictive performance 


```{r}

formulas = list(formula_1, formula_2, formula_3, formula_4, 
                formula_5, formula_6, formula_7, formula_8)

models = map(formulas, ~lm(.x, data = training, weights = fex_c))

evaluate_model = function(model) {
  
  prediction = predict(model, newdata = test)
  
  result = list("RMSE" = RMSE(pred = exp(prediction), obs = test$y_salary_m),
                "Prediction" = exp(prediction),
                "Resid" = test$y_salary_m-exp(prediction))
  return(result)
}

result = map(models, evaluate_model)
rmse = map_dbl(result, "RMSE")

```

For the specification with the lowest prediction error, explore those observations that seem to ”miss the mark.” To do so, compute the prediction errors
in the test sample, and examine its distribution. Are there any observations
in the tails of the prediction error distribution? Are these outliers potential
people that the DIAN should look into, or are they just the product of a
flawed model?

```{r}

check_model(models[[7]])

qqnorm(result[[7]][["Resid"]])

```
LOOCV. For the two models with the lowest predictive error in the previous section, calculate the predictive error using Leave-one-out-cross-validation
(LOOCV). Compare the results of the test error with those obtained with the
validation set approach and explore the potential links with the influence statistic. (Note: when attempting this subsection, the calculations can take a long
time, depending on your coding skills, plan accordingly!)


```{r}

plan(multisession, workers = parallel::detectCores() - 1)

# ctrl = trainControl(method = "LOOCV", verboseIter = T)
# 
# mod_7 = train(formula_7, data, method = 'lm', trControl = ctrl)
# mod_6 = train(formula_6, data, method = 'lm', trControl = ctrl)


loocv_model = function(i, data, model_formula) {
  train_data = data[-i, ]
  test_data = data[i, , drop = FALSE]

  modelo = lm(model_formula, data = train_data, weights = fex_c)  # Ajustar modelo
  prediction = predict(modelo, newdata = test_data)  # Predecir
  rmse = RMSE(test_data$y_salary_m, exp(prediction))  # Error cuadrático

  return(rmse)
}

rmse_mod7 = future_map_dbl(1:nrow(data), ~ loocv_model(.x, data, formula_7))
mean(rmse_mod7) # 745794.5

rmse_mod6 = future_map_dbl(1:nrow(data), ~ loocv_model(.x, data, formula_6))
mean(rmse_mod6) # 753385

plan(sequential)

```

Leverage statistic

```{r}
## RUN THE MODEL WITH ALL OBS

full_model <- lm(formula_7, data, weights = fex_c, na.action = na.omit)

X<- model.matrix(full_model, na.action = na.omit)
y <- model.response(model.frame(full_model))

beta_hat <- full_model$coefficients
  
## Calculate the inverse of  (X'X), call it G_inv
G_inv<- ginv(t(X)%*%X)

## and 1/1-hi
vec<- 1/(1-hatvalues(full_model))
  
N <- nrow(X)  # Number of observations
LOO <- numeric(N)  # To store the errors

  # Loop over each observation
for (i in 1:N) {
  # get the new beta
  new_beta<- beta_hat  - vec[i] * G_inv %*% as.vector(X[i, ]) * full_model$residuals[i]
    ## get the new error
    new_error<- (y[i]- (X[i, ] %*% new_beta))^2
    LOO[i]<-  new_error
  }
  
  looCV_error <- mean(LOO)
  sqrt(looCV_error)
```
