---
title: "Taller 1 - BD&ML Grupo 7"
author: "Grupo 7"
date: "2025-02-02"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
gc(rm(list = ls()))
knitr::opts_chunk$set(echo = TRUE)

require(pacman)
p_load(stringr, # html to text
       tidyverse, # tidy-data
       skimr, # summary data
       rvest, # Web scrapping
       dplyr, # managing tables
       corrplot,#Correlation plot
       reshape2, # Long format for table 
       stargazer,
       gridExtra # visualizing missing data
       ) 
```

## 1. Introduction

In the public sector, accurate reporting of individual income is
critical for computing taxes. However, tax fraud of all kinds has always
been a significant issue. According to the Internal Revenue Service
(IRS), about 83.6% of taxes are paid voluntarily and on time in the
US.1. One of the causes of this gap is the under-reporting of incomes by
individuals. An income predicting model could potentially assist in
agging cases of fraud that could lead to the reduction of the gap.
Furthermore, an income prediction model can help identify vulnerable
individuals and families that may need further assistance.

The objective of the problem set is to generate a predictive model on
income levels based on Bogota's information about employed adults (\>18
years old). Thus, we can predict levels of income and then estimate a
level of tax payment. For this objective the survey of 2018 GEIH from
DANE, will be used.

2.  Describe the data briefly, including its purpose, and any other
    relevant information.

## 2. Data proccessing

### 2.a) Data description

(Describe the process of acquiring the data and if there are any
restrictions to accessing/scraping these data.) For scrapping this data
we will use ignaciomsarmiento repository and rvest to read the htmls. As
this data frames are html tables, we will also use dplyr. [On latex]

### 2.b) Web Scrapping

<div>

```{r 2.b) Scrapping}
    # Link
    base <- "https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_"
    htmls <- paste0(base,"page_",1:10, ".html")

    tables_list <- list()
    for (url in htmls) {
      web_page <- read_html(url)
      table <- web_page %>%
        html_nodes("table.table-striped") %>%
        html_table(fill = TRUE)
      tables_list[[url]] <- table
    }

    db = bind_rows(tables_list) %>% select(-...1)
    rm(tables_list,web_page,table,base,htmls,url)
```

</div>

We will use the definition the variable "ocu" already included on the
data set which takes value 1 if the person is occupied (employed) and 0
otherwise. \### 2.c) Data cleaning process

#### 2.c.1) Filter and framming missing values

```{r Cleaning of data set, echo=FALSE}
# Filtering > 18 and occupied
db2 <- db %>% filter(age > 18, ocu == 1, dominio == "BOGOTA" )

#------------ FRAMING MISSING VALUES -------------------------------------------
Nobs <- nrow(db2) 
db_miss <- data.frame(
  Variable = names(db2),
  n_missing = colSums(is.na(db2)),
  p_missing = round(colSums(100*is.na(db2))/Nobs,2)
) 
rownames(db_miss) <- NULL
View(db_miss)

# Attaching question content
question <- read_html("https://ignaciomsarmiento.github.io/GEIH2018_sample/dictionary.html")
question <- question %>%
        html_nodes("table") %>%
        html_table(fill = TRUE)
question = bind_rows(question)
db_miss <- left_join(question, db_miss, by = "Variable")
db_miss <- db_miss %>% arrange(desc(p_missing)) %>% filter(p_missing != 0) 
db_miss

# As we can see, almost 99/177 variables have more than 20% of the sample missing. 
# Yet, many of these variables are associated with non-occupied people and are not
# relevant for this exercise. Others, are the same variable with different scales 
# (example: monthly salary vs hourly salary). Thus, we're only going to keep the following variables:

db2 <- db2 %>%
  select(y_ingLab_m, y_salary_m, y_total_m, ie,iees,  
         ingtotes, ingtotob, ingtot, y_bonificaciones_m, y_gananciaNeta_m, 
         y_gananciaNetaAgro_m, y_primaNavidad_m, y_primaServicios_m, y_primaVacaciones_m, 
         maxEducLevel, directorio, secuencia_p, orden, clase, dominio, mes, estrato1, 
         sex, age, oficio, fex_c, depto, fex_dpto, fweight, 
         informal,p6240,ocu,formal,informal,iees, imdi, impa, iof1, iof2, iof3h, iof6, isa,
         iees, imdies, impaes, ingtotes, iof1es, iof2es, iof3hes, iof3ies, iof6es, isaes,
         cclasnr4, cclasnr5, cclasnr2, cclasnr6, cclasnr7, cclasnr8,
         cclasnr11, cclasnr3,y_otros_m,y_vivienda_m,y_gananciaIndep_m)


skim(db2)
```

#### 2.c.2) Choosing variables

```{r Data exploration, echo=FALSE}
#------------ DATA CLEANING PROCESS: CONSISTENCY -------------------------------

# Occupied must be equal to the sum of formal + informal. 
Occupied_description <- db2 %>%
  group_by(p6240) %>%
  summarise(across(c(ocu, formal, informal), sum, na.rm = TRUE)) %>%
  ungroup() %>%
  add_row(p6240 = NA, 
          ocu = sum(.$ocu), 
          formal = sum(.$formal), 
          informal = sum(.$informal)) %>%
  mutate(Occupation = if_else(is.na(p6240), "Total",
                              recode(as.character(p6240),
                                     `6` = "Other activity",
                                     `5` = "Permanently unable to work",
                                     `4` = "Domestic work",
                                     `3` = "Studying",
                                     `2` = "Looking for work",
                                     `1` = "Working"))) %>%
  select(Occupation, ocu, formal, informal) %>%
  mutate(across(c(ocu, formal, informal), ~ . / last(ocu) * 100))
Occupied_description <- Occupied_description[-c(7),]
View(Occupied_description)

####  Export 
Occupied_description <- Occupied_description %>%
  mutate(across(c(ocu, formal, informal), ~ sprintf("%.2f", .x)))
stargazer(Occupied_description, summary = FALSE, type = "latex", rownames = FALSE)


# As we can see, most of income comes from either Working, Other activity or Domestic work. We're going to explore this income source individually. 

#------------ DATA CLEANING PROCESS: CHOOSING INCOME -------------------------------------------
### On the first place we're going to check differences of imputed variables vs observed variables. 
obs_vars <- c("ie", "imdi", "impa", "ingtot", "iof1", "iof2", "iof3h", "iof3i", "iof6", "isa")
imp_vars <- c("iees", "imdies", "impaes", "ingtotes", "iof1es", "iof2es", "iof3hes", "iof3ies", "iof6es", "isaes")
titles   <- c("Especie", "Labor income non-occupied", "Main Activity", "Total", 
              "Interes and dividends", "Retirement", "Household Aid", "Institutional Aid", 
              "Real State", "Second Activity")

# ----> Check if imputed variables cover missing values of observed income.
imp_vars2 <- c("iees", "imdies", "impaes", "iof1es", "iof2es", "iof3hes", "iof3ies", "iof6es", "isaes")
missing_report <- c("cclasnr4", "cclasnr5", "cclasnr2", "cclasnr6", "cclasnr7", "cclasnr8", "cclasnr8", "cclasnr11", "cclasnr3")

check <- function(db, imp_var, missing_var) {
  nrow(db2 %>% filter(!is.na(!!sym(imp_var)), !!sym(missing_var) != 1)) == 0
}
mapply(function(imp, miss) check(db2, imp, miss), imp_vars2, missing_report)
# (✓ Yes)

# ----> Check the dispersion of imputed vs observed
Stats <- mapply(function(ob, im, title) {
  obs <- db2[[ob]]; imp <- db2[[im]]
  q_obs <- quantile(obs, c(0.05, 0.25, 0.5, 0.75, 0.95), na.rm = TRUE)
  q_imp <- quantile(imp, c(0.05, 0.25, 0.5, 0.75, 0.95), na.rm = TRUE)
  
  data.frame(
    Statistic = c("Mean", "SD", "NA values", paste0("Quantile_", c(5, 25, 50, 75, 95))),
    Obs = c(mean(obs, na.rm = TRUE), sd(obs, na.rm = TRUE), sum(is.na(obs)), round(q_obs, 2)),
    Imp = c(mean(imp, na.rm = TRUE), sd(imp, na.rm = TRUE), sum(is.na(imp)), round(q_imp, 2))
  ) %>% rename_with(~ paste0(title, "_", sub("^(Obs|Imp)$", "\\1", .x)), Obs:Imp)
}, obs_vars, imp_vars, titles, SIMPLIFY = FALSE)

Obs_vs_Imp <- Reduce(function(x, y) merge(x, y, by = "Statistic", sort = FALSE), Stats)

# As we can see, imputed variables only cover certein NA values.
# ----> Imputing
for(i in seq_along(obs_vars)) {
  new_name <- paste0(obs_vars[i], "_oi")
  # Convert NULL to NA
  imp_val <- if (!is.null(db2[[imp_vars[i]]])) db2[[imp_vars[i]]] else rep(NA, nrow(db2))
  obs_val <- if (!is.null(db2[[obs_vars[i]]])) db2[[obs_vars[i]]] else rep(NA, nrow(db2))

  db2[[new_name]] <- ifelse(!is.na(imp_val), imp_val, obs_val)
}

# ----> Check constructed on data base vs Imputed+Observed: 

# For avoiding colinearity, we're going to check if two variables contain the same information.
# so that we only keep the most relevant one of the repeated information.

#  CORRELATION PLOT
db_temp <- db2 %>%
  mutate(
    Others = ie_oi,
    Total = ingtot_oi,
    `Informal` = ifelse(informal == 1, impa_oi, NA),
    `First Activity` = ifelse(formal == 1, impa_oi, NA),
    `Labor Income` = ifelse(p6240 == 4, impa_oi, NA)
  ) %>%
  select( Others, `Informal`, `First Activity`, Total,
    `Labor Income` = iof6_oi, `M. Others` = y_otros_m,
    `M. Formal Salary` = y_ingLab_m, `M. Total` = y_total_m,
    `M. Labor Income` = y_vivienda_m)

db_temp <- db_temp %>% 
  mutate_all(~ ifelse(is.na(.) | . == 0, 0, 1))
db_temp <-  db_temp %>%  select(which(apply(db_temp, 2, sd) > 0))

M <- cor(db_temp, use = "pairwise.complete.obs")
corrplot(
  M, method = "color",         # Use colored squares
  col = colorRampPalette(c("lightgray", "white", "blue"))(200),  
  tl.col = "black", tl.srt = 90,   tl.cex = 0.8, cl.cex = 0.9,                
  addCoef.col = "black", number.cex = 0.8,mar = c(2,2,2,2))

# Given this results, we will proceed filling NA values of labor 
# associated pre-constructed variables with the imputed data we elaborated.

#  Choosing total: 
t.test(db_temp$Total, db_temp$`M. Total`, paired = TRUE)

db_temp <-  db2 %>%  select(ingtot_oi,y_total_m,)
db_temp %>%
  summarise(across(everything(), ~ sum(is.na(.) | (. == 0), na.rm = TRUE)))


# Differences between the two group average 75.651 COP (significant) But ingtot_oi has less
# missing values/values at 0, thus, we're choosing this variable.


```

### 2.d) Descriptive analysis

```{r Data exploration, echo=FALSE}

# ----------------------DATA STATISTICAL DESCRIPTION ---------------------------
# Formal
# Filter for formal workers
Formal <- db2 %>%
  filter(formal == 1) %>%  # Keep only rows where formal == 1
  mutate(
    Occupation = if_else(is.na(p6240), "Total",
                         recode(as.character(p6240),
                                `6` = "Other activity",
                                `5` = "Permanently unable to work",
                                `4` = "Domestic work",
                                `3` = "Studying",
                                `2` = "Looking for work",
                                `1` = "Working"))
  ) %>%
  group_by(Occupation, sex) %>%  # Now also group by sex (gender)
  summarise(
    mean_age = mean(age, na.rm = TRUE),
    mean_y_total_m = mean(ingtot_oi, na.rm = TRUE),
    na_or_zero_percentage_total = sum(is.na(ingtot_oi) | ingtot_oi == 0) / n() * 100
  ) %>%
  ungroup()
View(Formal)

# Informal 
Informal <- db2 %>%
  filter(informal == 1) %>%  # Keep only rows where formal == 1
  mutate(
    Occupation = if_else(is.na(p6240), "Total",
                         recode(as.character(p6240),
                                `6` = "Other activity",
                                `5` = "Permanently unable to work",
                                `4` = "Domestic work",
                                `3` = "Studying",
                                `2` = "Looking for work",
                                `1` = "Working"))
  ) %>%
  group_by(Occupation, sex) %>%  # Now also group by sex (gender)
  summarise(
    mean_age = mean(age, na.rm = TRUE),
    mean_y_total_m = mean(ingtot_oi, na.rm = TRUE),
    na_or_zero_percentage_total = sum(is.na(ingtot_oi) | ingtot_oi == 0) / n() * 100
  ) %>%
  ungroup()
View(Informal)

#--- METERLE ACÁ NIVEL EDUCATIVO

```

#### 2.d.1) Filling NA's values

```{r Filling NA's, echo=FALSE}
# ------------------------- FILLING NA's VALUES ---------------------------------

# ----> MAXEDUC LEVEL: Max - Level of Education.
# Filling with mean as only 10% of values are missing
mode_edu <- as.numeric(names(sort(table(db2$maxEducLevel), decreasing = TRUE)[1]))
db2 <- db2  %>%
  mutate(maxEducLevel = ifelse(is.na(maxEducLevel) == TRUE, mode_edu , maxEducLevel))

# ----> LABORAL INCOME: FORMAL
# We'll fill "y_salary_m" null values with the observed+imputed data
# Creating the dummy variable for regressing over y_salary_m and y_ingLab_m

# Mincer approx.
db2$maxEducLevel<- factor(db2$maxEducLevel)
dummy_maxEducLevel <- as.data.frame(model.matrix(~ maxEducLevel - 1, data = db2))
db2 <- cbind(db2, dummy_maxEducLevel)
lM1 <- lm(y_salary_m ~ impa_oi + sex + maxEducLevel3 + maxEducLevel4 
          + maxEducLevel5 + maxEducLevel6 + maxEducLevel7 , data = db2)
db2$y_salary_m_P <- predict(lM1, newdata = db2)
db2$y_salary_m <- ifelse(is.na(db2$y_salary_m) & db2$p6240 == 1 & db2$formal == 1, 
                          db2$y_salary_m_P, 
                          db2$y_salary_m)

# ----> LABORAL INCOME: INFORMAL
# 21.5% missing. 
db2$y_gananciaIndep_m
lM1 <- lm(y_gananciaIndep_m ~ impa_oi + informal + sex + maxEducLevel3 
          + maxEducLevel4 + maxEducLevel5 + maxEducLevel6 + maxEducLevel7 , data = db2)
db2$y_gananciaIndep_m_P <- predict(lM1, newdata = db2)
db2$y_gananciaIndep_m <- ifelse(
  is.na(db2$y_gananciaIndep_m) & db2$p6240 == 1 & db2$informal == 1, 
  db2$y_gananciaIndep_m_P, 
  db2$y_gananciaIndep_m
)


# ----> FILTERED DATA BASE

# ----------------------COSAS QUE QUIERO METER ---------------------------------
# Set up a 2x5 grid with extra top space for a common legend
# Mirando a ver si sí va !! No he podido ponerle la leyenda en común FAK 
# par(mfrow = c(2, 5), oma = c(0, 0, 5, 0))  
# for(i in seq_along(obs_vars)) {
#   # Extract observed and imputado data
#   data_obs <- db2[[obs_vars[i]]]
#   data_imp <- db2[[imp_vars[i]]]
#   
#   # Exclude outliers based on 5th–95th percentiles
#   q_obs <- quantile(data_obs, probs = c(0.05, 0.95), na.rm = TRUE)
#   data_obs <- data_obs[data_obs >= q_obs[1] & data_obs <= q_obs[2]]
#   
#   q_imp <- quantile(data_imp, probs = c(0.05, 0.95), na.rm = TRUE)
#   data_imp <- data_imp[data_imp >= q_imp[1] & data_imp <= q_imp[2]]
#   
#   # Compute density estimates for each (line plot)
#   dens_obs <- density(data_obs, na.rm = TRUE)
#   dens_imp <- density(data_imp, na.rm = TRUE)
#   
#   # Set common x and y limits
#   xlim <- range(c(dens_obs$x, dens_imp$x), na.rm = TRUE)
#   ylim <- range(c(dens_obs$y, dens_imp$y), na.rm = TRUE)
#   
#   # Plot the observed density (black line)
#   plot(dens_obs, xlim = xlim, ylim = ylim,
#        main = titles[i], xlab = "", ylab = "Density",
#        col = "black", lwd = 2, type = "l")
#   # Overlay the imputado density (red line)
#   lines(dens_imp, col = "red", lwd = 2)
# }
# 
# # Create a new plotting region at the top for the common legend
# par(fig = c(0, 1, 0.92, 1), new = TRUE)
# plot(0, type = "n", axes = FALSE, xlab = "", ylab = "")
# legend("center", legend = c("Observed", "Imputed"),
#        col = c("black", "red"), lty = 1, horiz = TRUE, bty = "n", cex = 0.8)

# # Export differences Imputed - Observed
# cols <- 2:ncol(Obs_vs_Imp)
# obs_idx <- cols[seq(1, length(cols), by = 2)]
# imp_idx <- cols[seq(2, length(cols), by = 2)]
# Differences <- Obs_vs_Imp[, imp_idx] - Obs_vs_Imp[, obs_idx]
# colnames(Differences) <- titles
# Differences <- cbind(Statistic = Obs_vs_Imp[, 1], Differences)
# Differences <- Differences[-c(9),] 
# stargazer(Differences, summary = FALSE, type = "latex", rownames = FALSE)



# ---> Esto sí vale verga



#  As we already imputed values for extreme variables. Now wer're going to fill with the mean 
#  the incosistent data points and as we saw some people that repor
# income_sources <- c("ie_oi", "imdi_oi", "impa_oi","iof1_oi", "iof2_oi", "iof3h_oi", "iof3i_oi", "iof6_oi", "isa_oi")
# missing_report <- c("cclasnr4", "cclasnr5", "cclasnr2", "cclasnr6", "cclasnr7", "cclasnr8", "cclasnr8", "cclasnr11", "cclasnr3")
# 
# missing_stats <- list()
# missing_stats <- map2(income_sources, missing_report, function(income_var, missing_var) {
#   db2 %>%
#     group_by(p6240 = recode(as.character(p6240),
#                                 `6` = "Other activity",
#                                 `5` = "Permanently unable to work",
#                                 `4` = "Domestic work",
#                                 `3` = "Studying",
#                                 `2` = "Looking for work",
#                                 `1` = "Working")) %>%
#     summarise(
#       !!paste0(income_var, "Original_missing_count") := sum(is.na(.data[[income_var]])),
#       !!paste0(income_var, "_zero_count") := sum(.data[[income_var]] == 0, na.rm = TRUE),
#       !!paste0(income_var, "_Already imputated") := sum(.data[[missing_var]], na.rm = TRUE)
#     ) %>%
#     ungroup()
# })
# 
# # Merge all tables in the list by p6240
# merged_income_stats <- reduce(missing_stats, full_join, by = "p6240")
# write_xlsx(merged_income_stats, "merged_income_stats.xlsx")

rm(db3,db_miss,question, M, M_long,Nobs,mode_edu,quantiles,dummy_maxEducLevel,lM1,lM2,quant_table,y_ingLab_m_P, Occupied_description,imp_vars,obs_vars,titles,Stats,Obs_vs_Imp, imp_idx,imp_vars,obs_idx,cols, check)

```

## 3. Age wage profile

### 3.1 Running the regression

```{r Age wage Regression, echo=FALSE, warning=FALSE}
# ------------ SETING THE SEED  ------------------------------------------------
set.seed(1011)

# ------------ RUNNING THE REGRESSION ------------------------------------------
model_salary <- lm(log(y_salary_m) ~ age + I(age^2), data = db2)
summary(model_salary)

db2$y_salary_m_st <- scale(db2$y_salary_m)
db2$age_st <- scale(db2$age)
model_salary_st <- lm(log(y_salary_m_st) ~ age_st + I(age_st^2), data = db2)
summary(model_salary_st)

# ------------ EXPORTING TO LATEX R --------------------------------------------
stargazer(model_salary, type = "latex",
          title = "Monthly Income Salary relation with Age",
          covariate.labels = c("Age", "Age Squared"),
          dep.var.labels = "Labor monthly income*",
          star.cutoffs = c(0.1, 0.05, 0.01),
          ci = TRUE, ci.level = 0.95,
          out = "Regresion_P3.tex",
          notes = c("*Labor monthly income is measured in logarithmic form. Labor income includes formal and informal workers whose main income source is labor. 
          It includes tips and commissions. The variable was taken from the Gran Encuesta Integrada de Hogares (GEIH), and extreme values were imputed.",
          "Statistical significance levels: *** p<0.01, ** p<0.05, * p<0.1.")
)

stargazer(model_salary_st, type = "latex",
          title = "Monthly Income Salary relation with Age",
          covariate.labels = c("Age**", "Age Squared"),
          dep.var.labels = "Labor monthly income*",
          star.cutoffs = c(0.1, 0.05, 0.01),
          ci = TRUE, ci.level = 0.95,
          out = "Regresion_P3_sd.tex",
          notes = c("*Labor monthly income is measured in logarithmic form and was stadarized. Labor income includes formal and informal workers whose main income source is labor. 
          It includes tips and commissions. The variable was taken from the Gran Encuesta Integrada de Hogares (GEIH), and extreme values were imputed.",
          "** Age was stadarized",
          "Statistical significance levels: *** p<0.01, ** p<0.05, * p<0.1.")
)
# ------------ PLOTTING AGE-EARNINGS PROFILE -----------------------------------

# y_salary_m
ggplot(db2, aes(x = age, y = log(y_salary_m))) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", formula = y ~ x + I(x^2), color = "red") +
  ggtitle("Quadratic Regression: log(y_salary_m) ~ Age")

```

### 3.1 Evaluating residuals

Análisis preliminar de las regresiones:

-   Residuals vs Fitted: Podemos ver que en general los residuales se
    concentran etre -4 y 4. Que no parece existir una tendencia muy
    literal, aunque si la existiera sería negativa. En general los
    valores se concentran entre 13.5 y 14.0 es decir que el valor
    predicho de log(w) suele estar entre este rango para casi todas las
    observaciones, aunque con notables outliers para algunas
    observaciones. ¿Eso qué quiere decir? Que un cambio de 1 año de experiencia y su variación (cómo explico la experiencia al cuadrado?) genera un crecimiento (creo aproximado de 442.413 COP (COP?) 

-   Q-Q Plot: Claramente hay heterocedasticidad, presencia de colas
    psadas que nos indicanque existe una distribución asimetrica de los errores en 
    los cuantiles más extremos.
-   Scale-Location: Nuevamente sugiere que la varianza no es constante
-   Residuals vs Leverage: Nos muestra que existen observaciones con gran influencia. Tal cómo la 9452, 943, 9453 que pueden estar moviendo esa pendiente a niveles distorsionados.


```{r Evaluating residuals, echo=FALSE}

png("Reg1_P3.png", width = 800, height = 600) 
par(mfrow = c(2, 2))
plot(model_salary)
dev.off()

# Is age^2 worth it? Yes it is jeje 
model_salary2 <- lm(log(y_salary_m) ~ age, data = db2)
anova(model_salary , model_salary2)

## ------------------------ Further analysis on Levarage-----------------------

# leverage <- data.frame(row_num = 1:length(hatvalues(model_salary)))
# leverage <- leverage %>% mutate(leverage = hatvalues(model_salary))
# 
# res <- data.frame(row_num = 1:length(hatvalues(model_salary)))
# res <- res %>% mutate(residuals= model_salary$residuals)
# 
# ## PENDIENTE PORQUE NO TIENEN EL MISMO TAMAÑO POR LOS NA'S 
# N <- nrow(db2)
# db2$id<- seq(1 , N)
# db2$leverage <- hatvalues(model_salary) # Add leverage to db2
# a <- ggplot(db2, aes(y = leverage, x = id, color = y_salary_m, shape = as.factor(p6240))) +
#   geom_point() +
#   theme_bw() +
#   labs(x = "Observations", y = "Leverage", title = "")
# 
# residuals <- resid(model_salary) # Get the residuals
# b <- ggplot(db2, aes(y = leverage, x = residuals)) +
#   geom_point() +
#   theme_bw() +
#   labs(x = "Residuals", y = "Leverage", title = "")
# 
# grid.arrange(a, b, ncol = 2)

# ------------------------ DEALING WITH OUTLIERS--------------------------------
# p <- mean(db_int$leverage)
# # CUT OBSERVATIONS 3 STD AWAY from 
# cutt <- 3*p
# cutt
# db_int2 <-  db_int %>% 
#   dplyr:: filter(leverage<= cutt)
# # re run the model
# linear_model2<- lm(totalHoursWorked ~ ofic_ingLab + nmenores  +  nmenores*gender + H_Head + age + gender, data=db_int2  )
# 
# 
# stargazer(linear_model, linear_model2, type="text",
#           covariate.labels=c("Mean Ocu Income","N under 18","Male",
#                              "Hausehold Head","Age", "N under 18 x Male" ))
```


## 4. The gender earnings GAP
```{r Unconditional Gender Wage Gap}

# We are transforming the 'sex' variable to create a new 'Female' indicator where
#female = 1 for females (sex = 0) and Female = 0 for males (sex = 1) (According to data dictionary)

db2 <- db2 %>%
  mutate(female = ifelse(sex == 0, 1, 0)) 
head(db2)

model_unconditional <- lm(log(y_salary_m) ~ female, data = db2)
summary(model_unconditional)


#Gauss Marcov Theorem
#  Linearity Check:
ggplot(data = data.frame(Fitted = fitted(model_unconditional), Residuals = residuals(model_unconditional)), 
       aes(x = Fitted, y = Residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs Fitted Values", x = "Fitted Values", y = "Residuals")

# Exogeneity: Mean of residuals should be zero
mean(residuals(model_unconditional))

# Homoscedasticity: Breusch-Pagan Test
bptest(model_unconditional)  #P-value < 0.05 suggests rejecting the null hypothesis

# No Autocorrelation: Durbin-Watson Test
dwtest(model_unconditional) # P-value < 0.05 suggests rejecting the null hypothesis

```
The statistical significance of the model is evident from the extremely low p-values associated with both the Breusch-Pagan test for heteroscedasticity and the Durbin-Watson test for autocorrelation, these results indicate strong evidence against the null hypotheses, revealing violations in the homoscedasticity and independence of errors assumptions, respectively. Economically, the model's findings suggest that the gender variable (female) plays a significant role in explaining variations in the log-transformed salary, highlighting potential gender disparities in earnings, however, the presence of heteroscedasticity and autocorrelation implies that traditional inference may be flawed, necessitating robust econometric techniques to ensure the validity of the estimated effects.
